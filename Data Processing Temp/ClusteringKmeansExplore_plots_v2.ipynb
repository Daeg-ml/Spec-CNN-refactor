{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClusteringKmeansExplore_plots_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjoK0CooyLaq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe66dee4-9250-4fe9-b9ab-615ee7d222f9"
      },
      "source": [
        "#import pandas for data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZRy7GGfAdJl"
      },
      "source": [
        "def remove_repeat(file_ls,svd_fil):\n",
        "  #try to read in the saved data\n",
        "  try:\n",
        "    saved_s=pd.read_csv(svd_fil,index_col=0,header=None,squeeze=True)\n",
        "  except FileNotFoundError:\n",
        "    print('History file not found, all data files being processed')\n",
        "    return file_ls,False\n",
        "  \n",
        "  #somewhat clunky (but effective) method for checking if we've processed a file already\n",
        "  temp_s=pd.Series(file_ls)\n",
        "  tf_ls=temp_s.isin(saved_s)\n",
        "  idx_ls=[]\n",
        "  for i in range(0,len(tf_ls)):\n",
        "    if tf_ls[i]:\n",
        "      idx_ls.append(i)\n",
        "  temp_s.drop(index=idx_ls,inplace=True)\n",
        "  return temp_s.tolist(),True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwZJzvJeR7C_"
      },
      "source": [
        "def fname_ls_builder(fin_path):\n",
        "  #Find all files in data directory\n",
        "  from os import listdir\n",
        "  from os.path import isfile, join\n",
        "  return [f for f in listdir(fin_path) if isfile(join(fin_path,f))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DPkIZhuRB2Z"
      },
      "source": [
        "def peaks_cleaning(df):\n",
        "  #additional cleaning for peaks - drop NA rows, scale features\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  #scale data((X-mean)/std_dev)\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  sc = StandardScaler()\n",
        "  temp_df=pd.DataFrame(sc.fit_transform(df),columns=df.columns)\n",
        "  return temp_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMSJX_5rR_ve"
      },
      "source": [
        "def df_builder(fin_path,fname_ls):\n",
        "  #this function imports data from CSV files to one large dataframe\n",
        "\n",
        "  #create list to hold dataframes\n",
        "  df_ls=[]\n",
        "\n",
        "  #read in each file\n",
        "  for i in fname_ls:\n",
        "    temp_df=pd.read_csv(fin_path+i,index_col=0)\n",
        "    #print(i,'loaded\\n\\n')\n",
        "    df_ls.append(temp_df)\n",
        "\n",
        "  #create one large df\n",
        "  df=pd.concat(df_ls)\n",
        "  print(df)\n",
        "\n",
        "  #if peaks data, additional cleaning\n",
        "  if 'Peaks Only' in fin_path:\n",
        "    df=peaks_cleaning(df)\n",
        "\n",
        "  print('Master data set shape is',df.shape,'\\n\\n')\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6znpiKhys4I"
      },
      "source": [
        "def import_data(saved_data=None,explore=True):\n",
        "  #import data from CSV files\n",
        "  f_path=r'/content/drive/My Drive/ML Spectroscopy/Data/Preprocessed/'\n",
        "  m_path=r'/content/drive/My Drive/ML Spectroscopy/Models/'\n",
        "\n",
        "  cwt_in=f_path+r'Continuous Wavelet Transformation/Unlabeled/'\n",
        "  pk_in=f_path+r'Peaks Only/Unlabeled/'\n",
        "\n",
        "  cwt_fname_ls=fname_ls_builder(cwt_in)\n",
        "  pk_fname_ls=fname_ls_builder(pk_in)\n",
        "\n",
        "  if not explore:\n",
        "    cwt_fname_ls,fflag=remove_repeat(file_ls=cwt_fname_ls,svd_fil=saved_data)\n",
        "    pk_fname_ls,fflag=remove_repeat(file_ls=pk_fname_ls,svd_fil=saved_data)\n",
        "\n",
        "    if not pk_fname_ls:\n",
        "      raise ValueError('No new files')\n",
        "\n",
        "  cwt_df=df_builder(cwt_in,cwt_fname_ls)\n",
        "  pk_df=df_builder(pk_in,pk_fname_ls)\n",
        "\n",
        "  if explore:\n",
        "    return cwt_df,pk_df\n",
        "  return cwt_df,pk_df,fflag,cwt_fname_ls,pk_fname_ls\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uamWMnnsWEgl"
      },
      "source": [
        "def pca_process(X_cwt,X_pk):\n",
        "  from sklearn.decomposition import PCA\n",
        "  #let the fit function itself divide the data into batches\n",
        "  X_cwt_pca=PCA(n_components=3).fit_transform(X_cwt)\n",
        "  X_pk_pca=PCA(n_components=3).fit_transform(X_pk)\n",
        "  return X_cwt_pca,X_pk_pca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pypm_ezqeLzR"
      },
      "source": [
        "def kmeans_elbow(X,k,desc):\n",
        "  #import kmeans, plt, np, cdist, and metrics\n",
        "  from sklearn.cluster import KMeans\n",
        "  from sklearn import metrics \n",
        "  from scipy.spatial.distance import cdist \n",
        "  import numpy as np \n",
        "  #define empty lists for later use in elbow methods\n",
        "  X_distortions=[] \n",
        "  X_inertias=[]\n",
        "\n",
        "  #loop to run kmeans and find the elbow\n",
        "  for i in k:\n",
        "  #create a model with [1,10] clusters and fit to X, -1 n_jobs runs parallel operations\n",
        "    model=KMeans(n_clusters=i,n_jobs=-1).fit(X)\n",
        "\n",
        "    #measure and track distortion and inertia for elbow method\n",
        "    X_distortions.append(sum(np.min(cdist(X,model.cluster_centers_,'euclidean'),axis=1))/X.shape[0])\n",
        "    X_inertias.append(model.inertia_)\n",
        "\n",
        "\n",
        "    #count how many samples per cluster\n",
        "    check_set=model.predict(X)\n",
        "    check_list=[0,0,0,0,0,0,0,0,0,0]\n",
        "    for i in check_set:\n",
        "      check_list[i]+=1\n",
        "    print(check_list)\n",
        "\n",
        "  elbow_plt(k,X_distortions,desc,'Distortions')\n",
        "  elbow_plt(k,X_inertias,desc,'Inertias')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTXN7yMEvdWL"
      },
      "source": [
        "def elbow_plt(k,y_vals,desc,plt_type):\n",
        "  import matplotlib.pyplot as plt\n",
        "  #show elbow for distortions\n",
        "  plt.plot(k, y_vals, 'bx-') \n",
        "  plt.xlabel('Values of K') \n",
        "  plt.ylabel(plt_type) \n",
        "  plt.title(desc) \n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rf87P_i-iYz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f1238c4-358a-4ca0-b366-65c5b7c7c31d"
      },
      "source": [
        "def elbow_explore():\n",
        "  #import data\n",
        "  X_cwt,X_pk=import_data()\n",
        "\n",
        "  #store the indices mappings\n",
        "  X_cwt_og_map=X_cwt.index.values\n",
        "  X_pk_og_map=X_pk.index.values\n",
        "\n",
        "  #process with PCA\n",
        "  X_cwt_pca,X_pk_pca=pca_process(X_cwt,X_pk)\n",
        "\n",
        "  #define range for K for later use\n",
        "  k=range(2,11)\n",
        "  \n",
        "  #run elbow method on each data set\n",
        "  kmeans_elbow(X_cwt,k,'Elbow method for CWT')\n",
        "  kmeans_elbow(X_pk,k,'Elbow method for PK')\n",
        "  kmeans_elbow(X_cwt_pca,k,'Elbow method for CWT with PCA')\n",
        "  kmeans_elbow(X_pk_pca,k,'Elbow method for PK with PCA')\n",
        "\n",
        "'''uncomment to see elbow plots'''\n",
        "#elbow_explore()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'uncomment to see elbow plots'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOHX6Ft2VoCX"
      },
      "source": [
        "Due to the significantly clearer signals from the models built from the pk_ data rather than the cwt_ data, only the pk_ data will be used for labelling purposes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1eN58yGU1yt"
      },
      "source": [
        "def kmeans_spec(X,k,predict_store):\n",
        "  #import kmeans, plt, np, cdist, and metrics\n",
        "  from sklearn.cluster import KMeans\n",
        "  from sklearn import metrics \n",
        "  from scipy.spatial.distance import cdist \n",
        "  import numpy as np \n",
        "\n",
        "  #create a model with [1,10] clusters and fit to X, -1 n_jobs runs parallel operations\n",
        "  model=KMeans(n_clusters=k,n_jobs=-1).fit(X)\n",
        "\n",
        "\n",
        "  #count how many samples per cluster\n",
        "  check_set=model.predict(X)\n",
        "  print(check_set.shape)\n",
        "  check_list=[0,0,0,0,0,0,0,0,0,0]\n",
        "  for i in check_set:\n",
        "    check_list[i]+=1\n",
        "  print(check_list)\n",
        "  predict_store['pred_labels']=check_set\n",
        "  print(predict_store.shape)\n",
        "  return predict_store"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "271rOIIXXyEA"
      },
      "source": [
        "def spot_check_label(X,X2,X_labels):\n",
        "  import matplotlib.pyplot as plt\n",
        "  for i in range(0,len(X),10000):\n",
        "    #data spot check \n",
        "    plt.plot(X.columns.values,X.iloc[i])  \n",
        "    plt.title(X_labels['pred_labels'].iloc[i]) \n",
        "    plt.show()\n",
        "    plt.plot(X2.columns.values,X2.iloc[i])  \n",
        "    plt.title(X_labels['pred_labels'].iloc[i]) \n",
        "    plt.show()\n",
        "    print('\\n\\n\\n\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Puy0uSDafO0s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "374142dd-8f71-4d66-e971-cf13260e331d"
      },
      "source": [
        "def labeling_model(k):\n",
        "  #import data\n",
        "  saved_data=r'/content/drive/My Drive/ML Spectroscopy/Programs/Data Processing/Saved Lists/clustered.csv'\n",
        "  X_cwt,X_pk,fflag,cwt_ls,pk_ls=import_data(saved_data,explore=False)\n",
        "  \n",
        "  #store the new-index-to-original mapping and drop from large dataset\n",
        "  X_cwt_og_map=pd.DataFrame(X_cwt.index.values,columns=['og-idx'])\n",
        "  X_pk_og_map=pd.DataFrame(X_pk.index.values,columns=['og-idx'])\n",
        "\n",
        "  #process with PCA - data exploration suggests against pca\n",
        "  #X_cwt_pca,X_pk_pca=pca_process(X_cwt,X_pk)\n",
        "\n",
        "  #specific cluster model\n",
        "  X_pk_og_map=kmeans_spec(X_pk,k,X_pk_og_map)\n",
        "  print(X_pk_og_map)\n",
        "\n",
        "  #spot check clusters\n",
        "  spot_check_label(X_pk,X_cwt,X_pk_og_map)\n",
        "  X_pk_og_map.to_csv('/content/drive/My Drive/ML Spectroscopy/Data/Preprocessed/Clustered Data/granite_labels_5.csv')\n",
        "  print('file saved')\n",
        "\n",
        "  if fflag:\n",
        "    pd.Series(cwt_ls).to_csv(saved_data,mode='a')\n",
        "    pd.Series(pk_ls).to_csv(saved_data,mode='a')\n",
        "  else:\n",
        "    pd.Series(cwt_ls).to_csv(saved_data,mode='w')\n",
        "    pd.Series(pk_ls).to_csv(saved_data,mode='a')\n",
        "\n",
        "\n",
        "labeling_model(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-8cd884bc0e3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mlabeling_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-8cd884bc0e3b>\u001b[0m in \u001b[0;36mlabeling_model\u001b[0;34m(k)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m#import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0msaved_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr'/content/drive/My Drive/ML Spectroscopy/Programs/Data Processing/Saved Lists/clustered.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mX_cwt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_pk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfflag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcwt_ls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpk_ls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#store the new-index-to-original mapping and drop from large dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-b8cfa9d4144a>\u001b[0m in \u001b[0;36mimport_data\u001b[0;34m(saved_data, explore)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpk_fname_ls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No new files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mcwt_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwt_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcwt_fname_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No new files"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9qJIz61ds56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "f4905f92-2d8d-43d6-b039-c19540de37a1"
      },
      "source": [
        "print(df.head)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-e36cd32f078f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    }
  ]
}